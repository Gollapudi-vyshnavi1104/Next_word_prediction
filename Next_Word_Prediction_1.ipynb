{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XGTlBnzM2ze"
      },
      "source": [
        "# Next Word Prediction:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/next-word-prediction-with-nlp-and-deep-learning-48b9fe0a17bf- refernce"
      ],
      "metadata": {
        "id": "ePme_ggjRzQb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxXQXL5UM2zj"
      },
      "source": [
        "### Importing The Required Libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "043Ul-dzM2zj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Dataset: http://www.gutenberg.org/cache/epub/5200/pg5200.txt\n",
        "    Remove all the unnecessary data and label it as Metamorphosis-clean.\n",
        "    The starting and ending lines should be from the -\n",
        "    One morning, when Gregor Samsa woke &......first to get up and stretch out her young body"
      ],
      "metadata": {
        "id": "0eEkYk-eR8QS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfuZRQKVM2zk",
        "outputId": "2753cc68-bd9c-4b1f-ae43-eea3c5e1e74b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The First Line:  ï»¿One morning, when Gregor Samsa woke from troubled dreams, he found\n",
            "\n",
            "The Last Line:  first to get up and stretch out her young body.\n"
          ]
        }
      ],
      "source": [
        "file = open(\"metamorphosis_clean.txt\", \"r\", encoding = \"utf8\")\n",
        "lines = []\n",
        "\n",
        "for i in file:\n",
        "    lines.append(i)\n",
        "    \n",
        "print(\"The First Line: \", lines[0])\n",
        "print(\"The Last Line: \", lines[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb38eg7uM2zm"
      },
      "source": [
        "### Cleaning the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Xt2aEpXrM2zm",
        "outputId": "c477adad-3e2c-4622-a3f4-e443f9b86798"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.  He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections.  The bedding was hardly able to cover it and seemed ready to slide off any moment.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "data = \"\"\n",
        "\n",
        "for i in lines:\n",
        "    data = ' '. join(lines)\n",
        "    \n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
        "data[:360]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "t2LGln1iM2zm",
        "outputId": "4357ca8d-95c1-46fb-cf78-37a8613e9710"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One morning  when Gregor Samsa woke from troubled dreams  he found himself transformed in his bed into a horrible vermin   He lay on his armour like back  and if he lifted his head a little he could see his brown belly  slightly domed and divided by arches into stiff sections   The bedding was hardly able to cover it and seemed ready to slide off any moment   His many legs  pitifully thin compared with the size of the rest of him  waved about helplessly as he looked    What s happened to me   he'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "\n",
        "import string\n",
        "\n",
        "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
        "new_data = data.translate(translator)\n",
        "\n",
        "new_data[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "h-w2Y502M2zn",
        "outputId": "a71f1546-46ea-41c8-86f9-fac219b890cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on armour-like back, and if lifted head little could see brown belly, slightly domed divided by arches stiff sections. The bedding was hardly able to cover it seemed ready slide off any moment. His many legs, pitifully thin compared with the size of rest him, waved about helplessly as looked. \"What\\'s happened me?\" thought. It wasn\\'t dream. room, proper human room altho'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "z = []\n",
        "\n",
        "for i in data.split():\n",
        "    if i not in z:\n",
        "        z.append(i)\n",
        "        \n",
        "data = ' '.join(z)\n",
        "data[:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5IYEUznM2zo"
      },
      "source": [
        "### Tokenization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8cb-EikM2zo",
        "outputId": "f88dcd69-668d-4332-fc47-cd97e64a0bf0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[17, 53, 293, 2, 18, 729, 135, 730, 294, 8]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "# saving the tokenizer for predict function.\n",
        "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
        "\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxQZGqY0M2zp",
        "outputId": "2b4d57d1-2f52-4700-8e40-627fb2bd09ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2617\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tC8YogM_M2zp",
        "outputId": "de32ddef-29da-462d-8913-b26a9e839ba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Length of sequences are:  3889\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 17,  53],\n",
              "       [ 53, 293],\n",
              "       [293,   2],\n",
              "       [  2,  18],\n",
              "       [ 18, 729],\n",
              "       [729, 135],\n",
              "       [135, 730],\n",
              "       [730, 294],\n",
              "       [294,   8],\n",
              "       [  8, 731]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "sequences = []\n",
        "\n",
        "for i in range(1, len(sequence_data)):\n",
        "    words = sequence_data[i-1:i+1]\n",
        "    sequences.append(words)\n",
        "    \n",
        "print(\"The Length of sequences are: \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoX-qbFvM2zp"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "    X.append(i[0])\n",
        "    y.append(i[1])\n",
        "    \n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esn9N1QoM2zq",
        "outputId": "2c076cf9-8831-4146-97a2-74e35773d8b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Data is:  [ 17  53 293   2  18]\n",
            "The responses are:  [ 53 293   2  18 729]\n"
          ]
        }
      ],
      "source": [
        "print(\"The Data is: \", X[:5])\n",
        "print(\"The responses are: \", y[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IX23lpvSM2zq",
        "outputId": "9a81b144-14a5-45a8-8990-b714da969467"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "y[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzFk8qvwM2zq"
      },
      "source": [
        "### Creating the Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INma80j3M2zr"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=1))\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFIuHNg4M2zr",
        "outputId": "355df781-1883-4132-b6ac-c4cfe2846f92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1, 10)             26170     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 1, 1000)           4044000   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2617)              2619617   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,694,787\n",
            "Trainable params: 15,694,787\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO7Qdf-XM2zr"
      },
      "source": [
        "### Plot The Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "GWuuQPdgM2zr",
        "outputId": "7886fdcf-ea69-4a58-f4ad-32a420f7be8a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "keras.utils.plot_model(model, to_file='model.png', show_layer_names=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44VZQ3lIM2zr"
      },
      "source": [
        "### Callbacks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbV8we2FM2zs"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
        "    save_best_only=True, mode='auto')\n",
        "\n",
        "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
        "\n",
        "logdir='logsnextword1'\n",
        "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MY_90JfM2zs"
      },
      "source": [
        "### Compile The Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08Dy9ZwEM2zs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "813ebb98-d012-4550-b609-207a0476658d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOo2MRdYM2zs"
      },
      "source": [
        "### Fit The Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgrMbUqkM2zs",
        "outputId": "c3a8e86c-6803-40c7-817f-590b667e5689"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.8754\n",
            "Epoch 1: loss improved from inf to 7.87538, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 218ms/step - loss: 7.8754 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.8649\n",
            "Epoch 2: loss improved from 7.87538 to 7.86490, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 7.8649 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.8305\n",
            "Epoch 3: loss improved from 7.86490 to 7.83047, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 7.8305 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.6724\n",
            "Epoch 4: loss improved from 7.83047 to 7.67240, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 7.6724 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.4594\n",
            "Epoch 5: loss improved from 7.67240 to 7.45943, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 7.4594 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.2701\n",
            "Epoch 6: loss improved from 7.45943 to 7.27013, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 7.2701 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.1411\n",
            "Epoch 7: loss improved from 7.27013 to 7.14112, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 7.1411 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.0454\n",
            "Epoch 8: loss improved from 7.14112 to 7.04538, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 7.0454 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.9184\n",
            "Epoch 9: loss improved from 7.04538 to 6.91844, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 6.9184 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.6872\n",
            "Epoch 10: loss improved from 6.91844 to 6.68721, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 6.6872 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.4067\n",
            "Epoch 11: loss improved from 6.68721 to 6.40672, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 6.4067 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.1613\n",
            "Epoch 12: loss improved from 6.40672 to 6.16135, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 6.1613 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.9331\n",
            "Epoch 13: loss improved from 6.16135 to 5.93310, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 213ms/step - loss: 5.9331 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.7325\n",
            "Epoch 14: loss improved from 5.93310 to 5.73252, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 5.7325 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.5434\n",
            "Epoch 15: loss improved from 5.73252 to 5.54336, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 5.5434 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.3786\n",
            "Epoch 16: loss improved from 5.54336 to 5.37862, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 5.3786 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.2118\n",
            "Epoch 17: loss improved from 5.37862 to 5.21180, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 5.2118 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.0631\n",
            "Epoch 18: loss improved from 5.21180 to 5.06307, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 5.0631 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.9377\n",
            "Epoch 19: loss improved from 5.06307 to 4.93774, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 4.9377 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.8065\n",
            "Epoch 20: loss improved from 4.93774 to 4.80647, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 4.8065 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.7003\n",
            "Epoch 21: loss improved from 4.80647 to 4.70027, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 4.7003 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.5911\n",
            "Epoch 22: loss improved from 4.70027 to 4.59114, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 4.5911 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.4917\n",
            "Epoch 23: loss improved from 4.59114 to 4.49169, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 4.4917 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.3984\n",
            "Epoch 24: loss improved from 4.49169 to 4.39840, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 4.3984 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.3094\n",
            "Epoch 25: loss improved from 4.39840 to 4.30935, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 4.3094 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.2337\n",
            "Epoch 26: loss improved from 4.30935 to 4.23367, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 4.2337 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.1379\n",
            "Epoch 27: loss improved from 4.23367 to 4.13794, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 4.1379 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.0642\n",
            "Epoch 28: loss improved from 4.13794 to 4.06424, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 4.0642 - lr: 0.0010\n",
            "Epoch 29/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.9871\n",
            "Epoch 29: loss improved from 4.06424 to 3.98709, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 3.9871 - lr: 0.0010\n",
            "Epoch 30/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.8960\n",
            "Epoch 30: loss improved from 3.98709 to 3.89600, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 3.8960 - lr: 0.0010\n",
            "Epoch 31/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.7734\n",
            "Epoch 31: loss improved from 3.89600 to 3.77345, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 3.7734 - lr: 0.0010\n",
            "Epoch 32/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.6701\n",
            "Epoch 32: loss improved from 3.77345 to 3.67011, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 3.6701 - lr: 0.0010\n",
            "Epoch 33/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.5688\n",
            "Epoch 33: loss improved from 3.67011 to 3.56876, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 3.5688 - lr: 0.0010\n",
            "Epoch 34/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.4573\n",
            "Epoch 34: loss improved from 3.56876 to 3.45731, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 3.4573 - lr: 0.0010\n",
            "Epoch 35/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.3594\n",
            "Epoch 35: loss improved from 3.45731 to 3.35940, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 3.3594 - lr: 0.0010\n",
            "Epoch 36/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.2549\n",
            "Epoch 36: loss improved from 3.35940 to 3.25491, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 3.2549 - lr: 0.0010\n",
            "Epoch 37/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.1647\n",
            "Epoch 37: loss improved from 3.25491 to 3.16465, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 215ms/step - loss: 3.1647 - lr: 0.0010\n",
            "Epoch 38/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.0667\n",
            "Epoch 38: loss improved from 3.16465 to 3.06666, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 3.0667 - lr: 0.0010\n",
            "Epoch 39/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.9501\n",
            "Epoch 39: loss improved from 3.06666 to 2.95009, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 2.9501 - lr: 0.0010\n",
            "Epoch 40/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.8739\n",
            "Epoch 40: loss improved from 2.95009 to 2.87394, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 2.8739 - lr: 0.0010\n",
            "Epoch 41/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.8218\n",
            "Epoch 41: loss improved from 2.87394 to 2.82178, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 248ms/step - loss: 2.8218 - lr: 0.0010\n",
            "Epoch 42/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.7198\n",
            "Epoch 42: loss improved from 2.82178 to 2.71984, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 221ms/step - loss: 2.7198 - lr: 0.0010\n",
            "Epoch 43/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.6579\n",
            "Epoch 43: loss improved from 2.71984 to 2.65791, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 2.6579 - lr: 0.0010\n",
            "Epoch 44/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.6061\n",
            "Epoch 44: loss improved from 2.65791 to 2.60610, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 2.6061 - lr: 0.0010\n",
            "Epoch 45/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.5583\n",
            "Epoch 45: loss improved from 2.60610 to 2.55826, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 2.5583 - lr: 0.0010\n",
            "Epoch 46/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.5373\n",
            "Epoch 46: loss improved from 2.55826 to 2.53732, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 2.5373 - lr: 0.0010\n",
            "Epoch 47/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.4724\n",
            "Epoch 47: loss improved from 2.53732 to 2.47239, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 2.4724 - lr: 0.0010\n",
            "Epoch 48/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.4207\n",
            "Epoch 48: loss improved from 2.47239 to 2.42072, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 2.4207 - lr: 0.0010\n",
            "Epoch 49/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.3437\n",
            "Epoch 49: loss improved from 2.42072 to 2.34373, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 2.3437 - lr: 0.0010\n",
            "Epoch 50/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.2806\n",
            "Epoch 50: loss improved from 2.34373 to 2.28063, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 2.2806 - lr: 0.0010\n",
            "Epoch 51/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.2528\n",
            "Epoch 51: loss improved from 2.28063 to 2.25279, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 2.2528 - lr: 0.0010\n",
            "Epoch 52/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.2259\n",
            "Epoch 52: loss improved from 2.25279 to 2.22589, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 2.2259 - lr: 0.0010\n",
            "Epoch 53/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.1918\n",
            "Epoch 53: loss improved from 2.22589 to 2.19178, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 2.1918 - lr: 0.0010\n",
            "Epoch 54/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.1705\n",
            "Epoch 54: loss improved from 2.19178 to 2.17047, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 2.1705 - lr: 0.0010\n",
            "Epoch 55/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.1449\n",
            "Epoch 55: loss improved from 2.17047 to 2.14487, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 2.1449 - lr: 0.0010\n",
            "Epoch 56/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.1064\n",
            "Epoch 56: loss improved from 2.14487 to 2.10644, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 2.1064 - lr: 0.0010\n",
            "Epoch 57/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0401\n",
            "Epoch 57: loss improved from 2.10644 to 2.04012, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 2.0401 - lr: 0.0010\n",
            "Epoch 58/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.9870\n",
            "Epoch 58: loss improved from 2.04012 to 1.98703, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 1.9870 - lr: 0.0010\n",
            "Epoch 59/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.9740\n",
            "Epoch 59: loss improved from 1.98703 to 1.97398, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 1.9740 - lr: 0.0010\n",
            "Epoch 60/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.9438\n",
            "Epoch 60: loss improved from 1.97398 to 1.94379, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 1.9438 - lr: 0.0010\n",
            "Epoch 61/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.9514\n",
            "Epoch 61: loss did not improve from 1.94379\n",
            "61/61 [==============================] - 13s 212ms/step - loss: 1.9514 - lr: 0.0010\n",
            "Epoch 62/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8993\n",
            "Epoch 62: loss improved from 1.94379 to 1.89932, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 1.8993 - lr: 0.0010\n",
            "Epoch 63/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8871\n",
            "Epoch 63: loss improved from 1.89932 to 1.88708, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 1.8871 - lr: 0.0010\n",
            "Epoch 64/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8720\n",
            "Epoch 64: loss improved from 1.88708 to 1.87198, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 1.8720 - lr: 0.0010\n",
            "Epoch 65/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8593\n",
            "Epoch 65: loss improved from 1.87198 to 1.85928, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 1.8593 - lr: 0.0010\n",
            "Epoch 66/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8315\n",
            "Epoch 66: loss improved from 1.85928 to 1.83152, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 1.8315 - lr: 0.0010\n",
            "Epoch 67/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7909\n",
            "Epoch 67: loss improved from 1.83152 to 1.79088, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 217ms/step - loss: 1.7909 - lr: 0.0010\n",
            "Epoch 68/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7571\n",
            "Epoch 68: loss improved from 1.79088 to 1.75713, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 1.7571 - lr: 0.0010\n",
            "Epoch 69/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7397\n",
            "Epoch 69: loss improved from 1.75713 to 1.73973, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 221ms/step - loss: 1.7397 - lr: 0.0010\n",
            "Epoch 70/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7350\n",
            "Epoch 70: loss improved from 1.73973 to 1.73496, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 218ms/step - loss: 1.7350 - lr: 0.0010\n",
            "Epoch 71/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7276\n",
            "Epoch 71: loss improved from 1.73496 to 1.72763, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 1.7276 - lr: 0.0010\n",
            "Epoch 72/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7307\n",
            "Epoch 72: loss did not improve from 1.72763\n",
            "61/61 [==============================] - 13s 212ms/step - loss: 1.7307 - lr: 0.0010\n",
            "Epoch 73/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7022\n",
            "Epoch 73: loss improved from 1.72763 to 1.70216, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 1.7022 - lr: 0.0010\n",
            "Epoch 74/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6713\n",
            "Epoch 74: loss improved from 1.70216 to 1.67128, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 221ms/step - loss: 1.6713 - lr: 0.0010\n",
            "Epoch 75/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6769\n",
            "Epoch 75: loss did not improve from 1.67128\n",
            "61/61 [==============================] - 13s 212ms/step - loss: 1.6769 - lr: 0.0010\n",
            "Epoch 76/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7028\n",
            "Epoch 76: loss did not improve from 1.67128\n",
            "61/61 [==============================] - 13s 213ms/step - loss: 1.7028 - lr: 0.0010\n",
            "Epoch 77/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6632\n",
            "Epoch 77: loss improved from 1.67128 to 1.66325, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 1.6632 - lr: 0.0010\n",
            "Epoch 78/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6354\n",
            "Epoch 78: loss improved from 1.66325 to 1.63540, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 1.6354 - lr: 0.0010\n",
            "Epoch 79/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6245\n",
            "Epoch 79: loss improved from 1.63540 to 1.62448, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 1.6245 - lr: 0.0010\n",
            "Epoch 80/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5838\n",
            "Epoch 80: loss improved from 1.62448 to 1.58377, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 1.5838 - lr: 0.0010\n",
            "Epoch 81/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5639\n",
            "Epoch 81: loss improved from 1.58377 to 1.56393, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 1.5639 - lr: 0.0010\n",
            "Epoch 82/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5705\n",
            "Epoch 82: loss did not improve from 1.56393\n",
            "61/61 [==============================] - 13s 212ms/step - loss: 1.5705 - lr: 0.0010\n",
            "Epoch 83/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5249\n",
            "Epoch 83: loss improved from 1.56393 to 1.52488, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 219ms/step - loss: 1.5249 - lr: 0.0010\n",
            "Epoch 84/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5083\n",
            "Epoch 84: loss improved from 1.52488 to 1.50827, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 1.5083 - lr: 0.0010\n",
            "Epoch 85/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5000\n",
            "Epoch 85: loss improved from 1.50827 to 1.49998, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 1.5000 - lr: 0.0010\n",
            "Epoch 86/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5055\n",
            "Epoch 86: loss did not improve from 1.49998\n",
            "61/61 [==============================] - 14s 229ms/step - loss: 1.5055 - lr: 0.0010\n",
            "Epoch 87/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4930\n",
            "Epoch 87: loss improved from 1.49998 to 1.49296, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 1.4930 - lr: 0.0010\n",
            "Epoch 88/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5073\n",
            "Epoch 88: loss did not improve from 1.49296\n",
            "61/61 [==============================] - 13s 214ms/step - loss: 1.5073 - lr: 0.0010\n",
            "Epoch 89/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4712\n",
            "Epoch 89: loss improved from 1.49296 to 1.47125, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 1.4712 - lr: 0.0010\n",
            "Epoch 90/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4623\n",
            "Epoch 90: loss improved from 1.47125 to 1.46235, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 224ms/step - loss: 1.4623 - lr: 0.0010\n",
            "Epoch 91/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4589\n",
            "Epoch 91: loss improved from 1.46235 to 1.45887, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 1.4589 - lr: 0.0010\n",
            "Epoch 92/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4631\n",
            "Epoch 92: loss did not improve from 1.45887\n",
            "61/61 [==============================] - 13s 213ms/step - loss: 1.4631 - lr: 0.0010\n",
            "Epoch 93/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4540\n",
            "Epoch 93: loss improved from 1.45887 to 1.45405, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 221ms/step - loss: 1.4540 - lr: 0.0010\n",
            "Epoch 94/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4137\n",
            "Epoch 94: loss improved from 1.45405 to 1.41374, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 1.4137 - lr: 0.0010\n",
            "Epoch 95/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4035\n",
            "Epoch 95: loss improved from 1.41374 to 1.40355, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 1.4035 - lr: 0.0010\n",
            "Epoch 96/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3926\n",
            "Epoch 96: loss improved from 1.40355 to 1.39263, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 225ms/step - loss: 1.3926 - lr: 0.0010\n",
            "Epoch 97/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4064\n",
            "Epoch 97: loss did not improve from 1.39263\n",
            "61/61 [==============================] - 13s 216ms/step - loss: 1.4064 - lr: 0.0010\n",
            "Epoch 98/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3910\n",
            "Epoch 98: loss improved from 1.39263 to 1.39097, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 223ms/step - loss: 1.3910 - lr: 0.0010\n",
            "Epoch 99/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3722\n",
            "Epoch 99: loss improved from 1.39097 to 1.37219, saving model to nextword1.h5\n",
            "61/61 [==============================] - 14s 222ms/step - loss: 1.3722 - lr: 0.0010\n",
            "Epoch 100/100\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3578\n",
            "Epoch 100: loss improved from 1.37219 to 1.35782, saving model to nextword1.h5\n",
            "61/61 [==============================] - 13s 220ms/step - loss: 1.3578 - lr: 0.0010\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0f5793bf90>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "model.fit(X, y, epochs=100, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the Libraries\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Load the model and tokenizer\n",
        "\n",
        "model = load_model('nextword1.h5')\n",
        "tokenizer = pickle.load(open('tokenizer1.pkl', 'rb'))\n",
        "\n",
        "def Predict_Next_Words(model, tokenizer, text):\n",
        "    for i in range(3):\n",
        "        sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "        sequence = np.array(sequence)\n",
        "        \n",
        "        #preds = model.predict_classes(sequence)\n",
        "        preds=model.predict(sequence)\n",
        "        preds = np.argmax(preds)\n",
        "        print(preds)\n",
        "        predicted_word = \"\"\n",
        "        \n",
        "        for key, value in tokenizer.word_index.items():\n",
        "          #value == preds\n",
        "          if (value == preds).all():\n",
        "            predicted_word = key\n",
        "            break\n",
        "        \n",
        "        print(predicted_word)\n",
        "        return predicted_word"
      ],
      "metadata": {
        "id": "mBUdvVM-Ql2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# text1 = \"at the dull\"\n",
        "# text2 = \"collection of textile\"\n",
        "# text3 = \"what a strenuous\"\n",
        "# text4 = \"stop the script\"\n",
        "\n",
        "while(True):\n",
        "\n",
        "    text = input(\"Enter the string:\")\n",
        "    if text == \"stop the script\":\n",
        "        print(\"Ending The Program.....\")\n",
        "        break\n",
        "    else:\n",
        "        try:\n",
        "            text = text.split(\" \")\n",
        "            text = text[-1]\n",
        "            text = ''.join(text)\n",
        "            prediction = Predict_Next_Words(model, tokenizer, text)\n",
        "            print(prediction)\n",
        "            \n",
        "        except:\n",
        "            continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2METpU0SkqiQ",
        "outputId": "d409a6c1-f334-4626-9370-935c0e79ec08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the string:at the dull\n",
            "320\n",
            "weather\n",
            "........\n",
            "weather\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hy19az0rwsJy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}